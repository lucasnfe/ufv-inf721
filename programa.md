---
layout: page
title: Programa Pedagógico
permalink: /programa/
---

## Pré-requisitos

- **INF213: Estrutura de dados**
- **MAT135: Geometria Analítica e Álgebra Linear**
- **MAT140: Cálculo I**

    É esperado que os alunos estejam confortáveis com (1) programação em Python, (2) operações fundamentais com vetores e matrízes e (3) derivadas de funções compostas e multivariáveis. Além disso, espera-se que os alunos conheçam estruturas de dados fundamentais, bem como os algoritmos clássicos associados à elas. 


## Conteúdo Programático

1. **Introdução**

    História da Inteligência Artificial; machine Learning x deep Learning; o sucesso de deep learning; as sub-áreas de deep learning; tipos de redes neurais; tecnologias para implementação (python, jupyter notebook, numpy e frameworks).

2. **Aprendizado Supervisionado**

     Formulação matemática; problemas de classificação e regressão; conjuntos de dados, espaço de hipótese; treinamento, validação e teste; métricas de avaliação;

3. **Redes Neurais Rasas**

    Problemas linearmente separáveis; regressão logística, funções de custo; gradiente gescendente; implementação de redes neurais com numpy.

4. **Redes Neurais Profundas**

    Problemas linearmente não-separáveis; multilayer perceptron; funções de ativação; backpropagation; autograd; implementação de redes neurais com pytorch.

5. **Regularização**
    
    Generalização, sobreajuste e subajuste; aumentando conjuntos de dados; regularização l1 e l2; dropout; normalização de entrada; batch e camada.

6. **Otimização**

    Gradiente descendente estocástico, mini-batch, momentum, RMSprop, Adam, learning rate decay;

7. **Redes Neurais Convolucionais**
    
    Visão computacional, filtros e convoluções, padding e stride, convoluções em volumes, camadas de pooling, estudo de casos (AlexNet, ResNet, Inception, etc).

8. **Redes Neurais Recorrentes**

    Processamento de linguagem natural, redes neurais recorrentes (RNNs), explosão e esvanecimento de gradientes, gated recurrent unit (GRU), long short term memory (LSTM), RNNs bidirecionais.

9. **Mecanismos de Atenção e Transformers**

    Modelos encoder-decoder; autoatenção; atenção multi-head; transformers, estudo de casos (BERT e GPT).

10. **Modelos Generativos**

    Generative adversarial networks; variational autoencoders (VAEs); vector-quantized variational autoencoders (VQ-VAEs); diffusion models.